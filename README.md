# Energy Aware Decentralised Aggregation Mechanisms in Federated Learning
 This project introduces an energy-conscious, decentralized approach to Federated Learning (FL), focusing on enhancing privacy, scalability, and efficiency in distributed machine learning. It moves away from traditional FL models that depend on a central server for model aggregation, thereby avoiding potential single points of failure, latency, and energy inefficiency. The research delves into decentralized FL architectures where model updates are shared directly among clients via peer-to-peer communication. The project investigated various decentralized strategies, including gossip-based protocols, blockchain consensus, and hierarchical clustering, with a primary focus on optimizing energy consumption. To minimize communication overhead, techniques such as adaptive client participation, energy-efficient node selection, and model sparsification were employed. The work also addresses challenges like synchronization, security threats, and the absence of incentive mechanisms. The findings indicate that a decentralized, energy-aware FL system can serve as a robust and scalable substitute for centralized systems, especially in environments that are resource-constrained and privacy-sensitive. This research provides a basis for future explorations into intelligent coordination and sustainable distributed learning.
